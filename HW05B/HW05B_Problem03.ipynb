{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPuaJpsjsRlDFGtP7bVHUdr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### ***Problem01***\n","---"],"metadata":{"id":"0Rkfm5OQ2Gzd"}},{"cell_type":"code","source":["### Import library & data set\n","# library\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","import os\n","import re\n","import time\n","import sys\n","import nltk\n","import random\n","import numpy as np\n","import pandas as pd\n","from itertools import pairwise\n","import matplotlib.pyplot as plt\n","from nltk.tokenize import word_tokenize\n","from sklearn.decomposition import NMF\n","from sklearn.datasets import load_digits\n","from keras.preprocessing.text import Tokenizer\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","nltk.download('punkt')\n","# dataset\n","# 20ng\n","categories = ['alt.atheism', 'talk.religion.misc','comp.graphics', 'sci.space']\n","ng = fetch_20newsgroups(categories=categories)"],"metadata":{"id":"TxBpquU32Mig","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691429466276,"user_tz":240,"elapsed":45323,"user":{"displayName":"Hwijong Im","userId":"08386307815960807026"}},"outputId":"6b3c83e6-35ae-42b7-bb79-288ee4bf026f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["### 20ng dataset\n","text = []\n","for i in range(len(ng.data)):\n","  word = ng.data[i]\n","  word = re.sub(r\"[^a-zA-Z]\", \" \", word)\n","  words = re.sub(r\"\\s+\", \" \", word)\n","  text.append(words)\n","df = pd.DataFrame(text, columns=[\"text\"])\n","\n","# normalization\n","df[\"text\"] = df.text.str.strip()\n","df[\"text\"] = df.text.str.lower()\n","df_20ng = np.array(df[\"text\"])\n","\n","# tokenization\n","docs = []\n","for i in range(len(df_20ng)):\n","  words = list(word_tokenize(str(df_20ng[i])))\n","  docs.append(words)\n","X = docs[:100]"],"metadata":{"id":"WUi0py5s1Eq9","executionInfo":{"status":"ok","timestamp":1691429618533,"user_tz":240,"elapsed":5450,"user":{"displayName":"Hwijong Im","userId":"08386307815960807026"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["### gibbs\n","def gibbs_sampling_lda(documents,num_topics,num_iterations):\n","  # bag of words\n","  global vocab\n","  def create_vocab(docs): # vocab 나열\n","    vocab = set()\n","    for doc in docs:\n","      vocab.update(doc)\n","    return sorted(vocab)\n","\n","  # give index\n","  def doc_index(doc, vocab):\n","    return [vocab.index(word) for word in doc]\n","\n","  vocab = create_vocab(documents)\n","  num_docs = len(documents)\n","  \"\"\"vocab list에 각 단어 index 생성\"\"\"\n","  doc_indices = [doc_index(doc,vocab) for doc in documents]\n","\n","  # parameter rest\n","  \"\"\"topic matrix 생성, 계산에 필요한 matirx들 생성\"\"\"\n","  num_words = len(vocab)\n","  doc_topic_counts = np.zeros((num_docs,num_topics)) # docs topic matrix\n","  topic_word_counts = np.zeros((num_topics, num_words)) # number of words that topic contains\n","  topic_counts = np.zeros(num_topics) # 토픽 수 matrix\n","  doc_lengths = [len(doc) for doc in documents] # 각 document vocab 개수\n","  total_words = sum(doc_lengths) # 각 행 모든 단어 수\n","\n","  # random initialize\n","  \"\"\"\n","  doc 행 수 만큼 (d)\n","    doc 안의 단어 수만큼 (i)\n","      t = 행(d) 열(i) value(randint)\n","      w = 행(d) 열(i) value(index) / index를 통해 본래 단어를 찾아갈 수 있다\n","        단어 위치 추적 행렬 (w)\n","        실제 ouput (t)\n","  \"\"\"\n","  doc_assignments = [[random.randint(0,num_topics-1) for _ in range(len(doc))] for doc in documents]\n","  for d in range(num_docs):\n","    for i in range(len(documents[d])):\n","      t = doc_assignments[d][i] #\n","      w = doc_indices[d][i]\n","      doc_topic_counts[d,t] += 1\n","      topic_word_counts[t,w] += 1\n","      topic_counts[t] += 1\n","\n","  \"\"\"\n","  iteration 반복 수 만큼\n","    전체 doc 수 만큼\n","      doc 단어 수 만큼\n","        t, w 생성\n","        t = doc/words(random), randint=topic\n","\n","        doc_topic_counts = docs/topics, randint에 따라 맞는 topic에 += 1\n","        topic_word_counts = topics/num_words, 각 topic에 단어 수\n","        topic_counts = topics/count, 맞는 topic 나오면 -= 1\n","  \"\"\"\n","  for _ in range(num_iterations):\n","    for d in range(num_docs):\n","      for i in range(len(documents[d])):\n","        t = doc_assignments[d][i]\n","        w = doc_indices[d][i]\n","\n","        doc_topic_counts[d,t] -= 1\n","        topic_word_counts[t,w] -= 1\n","        topic_counts[t] -= 1\n","\n","         # posterior probability\n","        \"\"\"\n","        사후 확률 = doc/word 각각의 사후 확률을 계산하여 가장 높은 확률의 topic을 지정한다\n","        사후 확률에 맞춰서 matrix t (random choice) -> 실재 단어 한정 topic들의 단어 수, z\n","        (해당 토픽의 index 있는 단어 한정 토픽 수)*(해당 doc의 토픽 수) / (각 토픽 수 + 전체 단어 수)\n","        \"\"\"\n","        topic_probs = (topic_word_counts[:,w] + 1) * (doc_topic_counts[d] + 1) / (topic_counts + num_words)\n","        new_t = np.random.choice(range(num_topics), p=topic_probs / topic_probs.sum())\n","\n","        doc_topic_counts[d, new_t] += 1\n","        topic_word_counts[new_t, w] += 1\n","        topic_counts[new_t] += 1\n","        doc_assignments[d][i] = new_t\n","\n","  topic_word_probs = (topic_word_counts + 1) / (topic_counts[:, np.newaxis] + num_words)\n","\n","  return topic_word_probs"],"metadata":{"id":"Ti6GsZKT1S3V","executionInfo":{"status":"ok","timestamp":1691430628214,"user_tz":240,"elapsed":163,"user":{"displayName":"Hwijong Im","userId":"08386307815960807026"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# hyperparameters\n","num_topics = 5\n","num_iterations = 50\n","\n","# implement lda\n","topic_word_probs = gibbs_sampling_lda(X, num_topics, num_iterations)\n","\n","# result\n","for t, topic_probs in enumerate(topic_word_probs):\n","    top_words = [vocab[i] for i in np.argsort(topic_probs)[::-1][:5]]\n","    print(\"Topic {}: {}\".format(t+1, top_words))"],"metadata":{"id":"BKgBhrijXOrz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691430353885,"user_tz":240,"elapsed":105973,"user":{"displayName":"Hwijong Im","userId":"08386307815960807026"}},"outputId":"89885f3f-1c5a-4f8c-9551-29d778514c4a"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic 1: ['the', 'of', 'to', 'a', 'is']\n","Topic 2: ['de', 'van', 'het', 'een', 'en']\n","Topic 3: ['dm', 'koresh', 'tek', 'evil', 'almanac']\n","Topic 4: ['graphics', 'nasa', 'gov', 'systems', 'edu']\n","Topic 5: ['scodal', 'hendrix', 'clementine', 'information', 'email']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"j75EWCt3cgjI"},"execution_count":null,"outputs":[]}]}